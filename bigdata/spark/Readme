Filename: Readme (for Spark)

Q. What is Spark
A. Apache Spark is like that team of people, but for data. It's a system 
   used to process large amounts of data quickly by breaking it into smaller 
   parts and analyzing them in parallel (at the same time). So instead of 
   slowly going through everything one by one, Spark helps you do it in a 
   much faster and more efficient way.

   In short, Spark is a tool that speeds up the process of handling, 
   analyzing, and finding insights from big data. It can handle tasks 
   like searching, filtering, and sorting, making it easier to work with 
   massive datasets that might be too large for traditional tools.

Q. How to install Spark on bunch on Linux Machines
A. Apache Spark is free. It is open source software
   With a bunch of linux machines we can build a Spark Cluster
   A Spark cluster consists of a Master Node (the main controller) and 
   multiple Worker Nodes (which do the heavy lifting)

   Prerequisites:

    Linux Machines: You'll need a few Linux machines (both for the master 
      and worker nodes).
    Java: Apache Spark requires Java to run, so you need to have Java 
      installed on all your machines.
    SSH Access: You should be able to SSH into each machine 
    Hadoop (optional): If you're planning to use HDFS 
      (Hadoop Distributed File System) with Spark, Hadoop should be 
      installed, Spark can also work without Hadoop, use S3 or local FS

   How to Use:
   $/opt/spark/bin/spark-submit --master spark://<master-node-ip>:7077 --class <your-main-class> <your-application-jar>.jar

   If you have Python and PySpark, we can run python jobs as well

Q. Assume we have millions of employee records, need to generate userid
   Write some Java code

A. Assume we have records like
    John,Doe,123-45-6789
    Jane,Marie,Smith,234-56-7890
   
   The login ID is constructed as follows:

    - Firstname initial
    - Middle name initial (if present, otherwise 'x')
    - Lastname initial
    - Last 4 digits of SSN

   Steps to implement:

    - Read the employee records from a file using Spark's SparkContext or Dataset.
    - Transform each record to generate a login ID based on the specified rules.
    - Write the results back to a file or database.

   Code
   ----
//start
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.SparkConf;

import java.util.Arrays;
import java.util.List;

public class EmployeeLoginGenerator {

    public static void main(String[] args) {
        // Initialize SparkConf and JavaSparkContext
        SparkConf conf = new SparkConf().setAppName("EmployeeLoginGenerator").setMaster("spark://<master-node-ip>:7077");
        JavaSparkContext sc = new JavaSparkContext(conf);

        // Read the employee records from a text file
        String inputFile = "hdfs://<your-path>/employee_records.txt"; // or local file path
        JavaRDD<String> employeeRecords = sc.textFile(inputFile);

        // Process each record to create login IDs
        JavaRDD<String> loginIds = employeeRecords.map(record -> {
            // Split the employee record by comma
            String[] fields = record.split(",");

            // Handle records that might have 3 or 4 fields (if middle name is present or not)
            String firstName = fields[0];
            String middleName = fields.length > 3 ? fields[1] : "x"; // If middle name is not there, use "x"
            String lastName = fields[fields.length > 3 ? 2 : 1];
            String ssn = fields[fields.length > 3 ? 3 : 2]; // SSN is always the last field

            // Extract the initials and last 4 digits of SSN
            String firstInitial = firstName.substring(0, 1).toLowerCase();
            String middleInitial = middleName.substring(0, 1).toLowerCase();
            String lastInitial = lastName.substring(0, 1).toLowerCase();
            String ssnLast4 = ssn.substring(ssn.length() - 4);

            // Generate the login ID
            return firstInitial + middleInitial + lastInitial + ssnLast4;
        });

        // Save the login IDs to a file (HDFS, S3, or local)
        String outputFile = "hdfs://<your-path>/output_login_ids.txt";
        loginIds.saveAsTextFile(outputFile);

        // Stop the Spark context
        sc.stop();
    }
}
//end

   Explanation of the Code:

    SparkContext Setup:
        SparkConf initializes the Spark configuration with the app name and 
        master node (use the appropriate master URL).
        JavaSparkContext is used to interact with the Spark cluster.

    Reading the Data:
        The textFile method is used to load the employee records file into an 
        RDD. The file can be stored on HDFS, local file system, or S3 
        depending on your setup.

    Processing Each Record:
        map function is used to process each record in the RDD. Each record 
        (employee data) is split by commas to extract the first name, middle 
        name, last name, and SSN.
        The initials are extracted from the first, middle, and last names 
        (if present), and the last 4 digits of SSN are obtained using substring.

    Saving the Results:
        The results (login IDs) are saved into an output text file using saveAsTextFile.

    Stopping the Spark Context:
        Finally, the stop method is called to stop the Spark context after processing.

   Submit the Job to Spark Cluster:
     $spark-submit --class EmployeeLoginGenerator --master spark://<master-node-ip>:7077 /path/to/EmployeeLoginGenerator.jar

Q. What other methods does javasparkcontext support that helps to process 
   large amounts of data
A. JavaSparkContext is the entry point to interact with a Spark cluster 
   process large datasets efficiently in parallel across a cluster of machines. 

   textFile()

    Purpose: Reads data from a file into an RDD (Resilient Distributed Dataset), which can be processed in parallel.

   parallelize()

    Purpose: Converts a collection (like a List or Array) into an RDD that can be processed in parallel.

   wholeTextFiles()

    Purpose: Reads multiple files into an RDD of key-value pairs.

   union()

    Purpose: Combines two or more RDDs into a single RDD.

   filter()

    Purpose: Filters the data in an RDD based on a given condition

   map()

    Purpose: Transforms each element of the RDD using a provided function.

   flatMap()

    Purpose: Similar to map(), but each input element can produce zero or more output elements (unlike map(), which produces one output per input).

   reduce()

    Purpose: Performs a reduction on the dataset, aggregating the data (e.g., sum, max, min, etc.) across partitions.

   groupByKey()

    Purpose: Groups the data in an RDD by key, which can be used for further aggregation.

   join()/sortBy()/cache() / persist()/saveAsTextFile()/count()/take()
